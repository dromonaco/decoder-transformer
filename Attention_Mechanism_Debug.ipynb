{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1394dae-2485-423e-92e4-b8cb78be1931",
   "metadata": {},
   "source": [
    "You have moved from Batch Mode (compile $\\rightarrow$ run $\\rightarrow$ wait $\\rightarrow$ read logs) to Interactive Mode (REPL with persistence).\n",
    "For Deep Learning specifically, this is a massive shift. In your Main.hs, if you want to know \"What is the shape of the Query tensor inside the attention head?\", you have to insert a print statement, recompile the whole project, and run it. In Jupyter, you can inspect it instantly.\n",
    "\n",
    "At the bottom there are 4 concrete things you can do right now with your specific Transformer implementation that you couldn't do easily before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34088e46-0e01-49ae-96f2-04847732ec10",
   "metadata": {},
   "outputs": [],
   "source": [
    ":m +Prelude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ec35e96-0c8a-4bcc-a3ac-0d0ab7f66381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Torch (DType, Device, Parameter, Tensor, asTensor, defaultOpts, makeIndependent, ones, ones', toDependent, zeros, zeros')\n",
    "import qualified Torch as Th\n",
    "import qualified Torch.Autograd as TA\n",
    "import qualified Torch.Functional as F\n",
    "import qualified Torch.Functional.Internal as FI\n",
    "import qualified Torch.NN as NN\n",
    "import qualified Torch.Optim as Optim\n",
    "import qualified Torch.Serialize as Serialize\n",
    "import qualified Data.Set.Ordered as OSet\n",
    "import qualified Control.Foldl as L\n",
    "import qualified Data.Text as T\n",
    "import Data.Maybe (fromMaybe)\n",
    "import Text.Printf\n",
    "import IHaskell.Display\n",
    "import DecoderTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "851fd6f8-69f2-480d-8b0f-27f5e58adde5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor Float [2,2] [[ 1.0000   ,  1.0000   ],\n",
       "                    [ 1.0000   ,  1.0000   ]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t = ones' [2, 2]\n",
    "print t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b65cd23e-9d03-4d30-88cf-9c40626ca596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor Float [3,3] [[ 0.7668   ,  1.1573   ,  1.6517   ],\n",
       "                    [ 0.7668   ,  1.1573   ,  1.6517   ],\n",
       "                    [ 0.7668   ,  1.1573   ,  1.6517   ]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "let a = ones' [3, 3]\n",
    "b <- Th.randIO' [3, 3]\n",
    "-- This triggers the underlying C++ matrix multiplication\n",
    "let c = Th.matmul a b\n",
    "print c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38aa8b4a-190c-4a5d-ab3a-5ffc357e5085",
   "metadata": {},
   "source": [
    "## Four things that can be done now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c77ad9-91a0-4236-a82f-c9a7e15e6785",
   "metadata": {},
   "source": [
    "### 1. \"Surgical\" Shape Debugging\n",
    "Transformers are notorious for silent failures where dimensions are permuted (e.g., swapping Sequence Length and Embedding Dimension).\n",
    "In Main.hs, forwardMHA is a black box. In Jupyter, you can copy the body of that function into a cell and inspect the intermediate tensors step-by-step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2ae8f00-be72-41a9-be23-8e135e50ffa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,10,64]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "-- 1. Initialize the full model structure\n",
    "-- We use a small vocab size (e.g., 100) just for testing shapes\n",
    "let vocabSize = 100\n",
    "model <- initModel vocabSize\n",
    "\n",
    "-- 2. Extract just the Attention layer from the first block\n",
    "-- Your model has 'layers', which is a list of TransformerBlocks\n",
    "let firstBlock = head (layers model)\n",
    "let mhaLayer = attention firstBlock\n",
    "\n",
    "-- 3. Create dummy input [Batch=2, Seq=10, Dim=64]\n",
    "-- (Matches the embedDim=64 in your code)\n",
    "let dummyInput = ones' [2, 10, 64]\n",
    "\n",
    "-- 4. Run ONLY the Multi-Head Attention forward pass\n",
    "-- This lets you see the output shape of just that specific component\n",
    "let output = forwardMHA mhaLayer dummyInput\n",
    "\n",
    "print (Th.shape output)\n",
    "-- Should be [2, 10, 64]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c513a069-e5a0-4405-b3c7-5ac18d8bc1d6",
   "metadata": {},
   "source": [
    "### 2. Interactive Generation (The \"Chatbot\" Feel)\n",
    "Currently, your generateSequence function runs at the end of training. If you want to see how the model responds to \"The wizard cast a\", you have to re-run the program.\n",
    "In Jupyter, you can load the model once and then generate endless variations instantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6099977-c9d2-4375-b764-71ef1e713a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "{-# LANGUAGE OverloadedStrings #-}\n",
    "\n",
    "-- 1. Define both files used during training\n",
    "let trainFile = \"rpg-training-tokenized.txt\"\n",
    "let evalFile = \"rpg-evaluation-tokenized.txt\"\n",
    "\n",
    "-- 2. Load both files\n",
    "vocabParts <- traverse buildVocabFromFile [trainFile, evalFile]\n",
    "\n",
    "-- 3. Merge them AND add the [PAD] token (Crucial step!)\n",
    "-- This matches the logic in your compiled program\n",
    "let vocab = L.fold (L.Fold (OSet.|<>) (OSet.singleton \"[PAD]\") id) vocabParts\n",
    "let vocabSize = OSet.size vocab\n",
    "\n",
    "-- 4. Initialize an empty model structure\n",
    "model <- initModel vocabSize\n",
    "\n",
    "-- 2. Load the trained weights from the file created by 'stack run'\n",
    "loadedTensors <- Serialize.load \"rpg_model.pt\"\n",
    "\n",
    "-- 3. Hydrate the model\n",
    "loadedParams <- mapM makeIndependent loadedTensors\n",
    "let trainedModel = Th.replaceParameters model loadedParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8da7306d-981e-4cfb-9436-7fec6c4e244e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prompt: The wizard -> \n",
       " = 4\n",
       " c eval BinaryData = \" encoding\n",
       " c if BinaryData = 20\n",
       " P E\n",
       " R * *** Exported because , internally used by RPGUNIT tests ***\n",
       " P LIKE SENT TO YOUR E\n",
       " P HTTP_SetfileCCSID ...\n",
       " // Validate types\n",
       " P E\n",
       "\n",
       "Done."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generateSequence trainedModel vocab 50 \"The wizard\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f4c594-3659-4308-b9b0-01e6f9241f28",
   "metadata": {},
   "source": [
    "### 3. Visualizing Attention Weights\n",
    "This is the \"Killer Feature\" of Transformers. Your current forwardMHA calculates attnWeights but discards them after the matrix multiplication.\n",
    "In Jupyter, you can redefine a \"Debug\" version of forwardMHA that returns the weights, pass your data through it, and actually look at the probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e5b68fa-f342-4ff2-9e86-41ba73bb8ce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor Float [5,5] [[ -0.0000, -1.0000e9   , -1.0000e9   , -1.0000e9   , -1.0000e9   ],\n",
       "                    [ -0.0000,  -0.0000, -1.0000e9   , -1.0000e9   , -1.0000e9   ],\n",
       "                    [ -0.0000,  -0.0000,  -0.0000, -1.0000e9   , -1.0000e9   ],\n",
       "                    [ -0.0000,  -0.0000,  -0.0000,  -0.0000, -1.0000e9   ],\n",
       "                    [ -0.0000,  -0.0000,  -0.0000,  -0.0000,  -0.0000]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "-- Verify your causal mask works\n",
    "let mask = makeCausalMask 5 (Th.Device Th.CPU 0) Th.Float\n",
    "print mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57a99ae6-2e27-4164-a053-511895300b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "{-# LANGUAGE OverloadedStrings #-}\n",
    "\n",
    "-- A helper to visualize a 2D attention matrix as an HTML Table\n",
    "visualizeAttention :: Th.Tensor -> IO ()\n",
    "visualizeAttention t = do\n",
    "  -- Convert tensor to list of lists (assuming 2D [Seq, Seq])\n",
    "  let rows = (Th.asValue t :: [[Float]])\n",
    "  \n",
    "  -- Helper to generate the HTML for a single cell\n",
    "  let cell val = \n",
    "        let \n",
    "           -- Background: Blue with opacity matching the attention weight\n",
    "           bgStyle = printf \"background-color: rgba(0, 0, 255, %.2f)\" val :: String\n",
    "           \n",
    "           -- Text Color: White if background is dark/intense (> 0.5), Black otherwise\n",
    "           -- This prevents \"White Text on White Background\" issues for low values\n",
    "           textColor = if val > 0.5 then \"white\" else \"black\" :: String\n",
    "           \n",
    "        in printf \"<td style='%s; color: %s; width: 40px; height: 40px; border: 1px solid #ddd; text-align: center; font-size: 12px;'>%.2f</td>\" bgStyle textColor val\n",
    "  \n",
    "  let rowHtml r = \"<tr>\" ++ concatMap cell r ++ \"</tr>\"\n",
    "  let tableHtml = \"<table style='border-collapse: collapse; font-family: sans-serif;'>\" ++ concatMap rowHtml rows ++ \"</table>\"\n",
    "  \n",
    "  printDisplay $ Display [html tableHtml]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91e8f529-3dce-4b25-81cf-3895c0def4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "{-# LANGUAGE RecordWildCards #-}\n",
    "\n",
    "-- Returns (Output, AttentionWeights)\n",
    "forwardMHADebug :: MultiHeadAttention -> Th.Tensor -> (Th.Tensor, Th.Tensor)\n",
    "forwardMHADebug MultiHeadAttention {..} x =\n",
    "  let \n",
    "      -- 1. Linear Projections\n",
    "      q = NN.forward mhaLinearQ x\n",
    "      k = NN.forward mhaLinearK x\n",
    "      v = NN.forward mhaLinearV x\n",
    "\n",
    "      headDim = mhaEmbedDim `Prelude.div` mhaHeads\n",
    "      batch = head (Th.shape x)\n",
    "      seqLength = Th.shape x !! 1\n",
    "\n",
    "      -- 2. Reshape\n",
    "      viewShape = [batch, seqLength, mhaHeads, headDim]\n",
    "      q' = F.transpose (F.Dim 1) (F.Dim 2) $ Th.reshape viewShape q\n",
    "      k' = F.transpose (F.Dim 1) (F.Dim 2) $ Th.reshape viewShape k\n",
    "      v' = F.transpose (F.Dim 1) (F.Dim 2) $ Th.reshape viewShape v\n",
    "\n",
    "      -- 3. Scores\n",
    "      kT = F.transpose (F.Dim 2) (F.Dim 3) k'\n",
    "      scoresRaw = F.matmul q' kT\n",
    "      dk = Th.asTensor (fromIntegral headDim :: Float)\n",
    "      scoresScaled = scoresRaw / F.sqrt dk\n",
    "\n",
    "      -- 4. Mask (Simplified for debug: Optional)\n",
    "      -- For simple visualization, we can skip the mask or apply it if needed.\n",
    "      -- If you want to see the triangle, include the masking logic here.\n",
    "      \n",
    "      -- 5. Softmax -> THIS IS THE HEATMAP\n",
    "      attnWeights = F.softmax (F.Dim 3) scoresScaled\n",
    "\n",
    "      -- 6. Context\n",
    "      context = F.matmul attnWeights v'\n",
    "      contextT = F.transpose (F.Dim 1) (F.Dim 2) context\n",
    "      contextReshaped = Th.reshape [batch, seqLength, mhaEmbedDim] contextT\n",
    "      \n",
    "      finalOut = NN.forward mhaLinearOut contextReshaped\n",
    "   in (finalOut, attnWeights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4efbce56-ba6c-4c50-b06d-96b1123497ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "forwardEmbedding :: TransformerModel -> Tensor -> Tensor\n",
    "forwardEmbedding TransformerModel {..} input = \n",
    "  let w = toDependent embedWeights\n",
    "      emb = F.embedding False False w paddingIdx input\n",
    "      pos = toDependent posEncoding\n",
    "      -- Slice pos to match input sequence length if needed, or broadcast\n",
    "      -- Simple broadcasting works if seqLen matches\n",
    "  in emb + pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c279566f-f809-41e2-85dc-84a5eca46f62",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "CppStdException e \"The size of tensor a (5) must match the size of tensor b (64) at non-singleton dimension 1\\nException raised from infer_size_impl at ../aten/src/ATen/ExpandUtils.cpp:31 (most recent call first):\\nframe #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xb0 (0x76d53380a120 in /home/dave/.cache/libtorch/2.5.0/linux-x86_64/cpu/lib/libc10.so)\\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0xfa (0x76d5337ada5a in /home/dave/.cache/libtorch/2.5.0/linux-x86_64/cpu/lib/libc10.so)\\nframe #2: at::infer_size_dimvector(c10::ArrayRef<long>, c10::ArrayRef<long>) + 0x3d4 (0x76d5200c39a4 in /home/dave/.cache/libtorch/2.5.0/linux-x86_64/cpu/lib/libtorch_cpu.so)\\nframe #3: at::TensorIteratorBase::compute_shape(at::TensorIteratorConfig const&) + 0xc0 (0x76d52017eab0 in /home/dave/.cache/libtorch/2.5.0/linux-x86_64/cpu/lib/libtorch_cpu.so)\\nframe #4: at::TensorIteratorBase::build(at::TensorIteratorConfig&) + 0x6d (0x76d52017fd9d in /home/dave/.cache/libtorch/2.5.0/linux-x86_64/cpu/lib/libtorch_cpu.so)\\nframe #5: at::TensorIteratorBase::build_borrowing_binary_op(at::TensorBase const&, at::TensorBase const&, at::TensorBase const&) + 0x100 (0x76d5201811e0 in /home/dave/.cache/libtorch/2.5.0/linux-x86_64/cpu/lib/libtorch_cpu.so)\\nframe #6: at::meta::structured_add_Tensor::meta(at::Tensor const&, at::Tensor const&, c10::Scalar const&) + 0x32 (0x76d5204e0432 in /home/dave/.cache/libtorch/2.5.0/linux-x86_64/cpu/lib/libtorch_cpu.so)\\nframe #7: <unknown function> + 0x2f28536 (0x76d521728536 in /home/dave/.cache/libtorch/2.5.0/linux-x86_64/cpu/lib/libtorch_cpu.so)\\nframe #8: <unknown function> + 0x2f2864b (0x76d52172864b in /home/dave/.cache/libtorch/2.5.0/linux-x86_64/cpu/lib/libtorch_cpu.so)\\nframe #9: at::_ops::add_Tensor::redispatch(c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, c10::Scalar const&) + 0x91 (0x76d521202b21 in /home/dave/.cache/libtorch/2.5.0/linux-x86_64/cpu/lib/libtorch_cpu.so)\\nframe #10: <unknown function> + 0x501f0b9 (0x76d52381f0b9 in /home/dave/.cache/libtorch/2.5.0/linux-x86_64/cpu/lib/libtorch_cpu.so)\\nframe #11: <unknown function> + 0x501f6ee (0x76d52381f6ee in /home/dave/.cache/libtorch/2.5.0/linux-x86_64/cpu/lib/libtorch_cpu.so)\\nframe #12: at::_ops::add_Tensor::call(at::Tensor const&, at::Tensor const&, c10::Scalar const&) + 0x182 (0x76d52124b2d2 in /home/dave/.cache/libtorch/2.5.0/linux-x86_64/cpu/lib/libtorch_cpu.so)\\nframe #13: inline_c_Torch_Internal_Unmanaged_Native_Native0_91 + 0x50 (0x76d4e94a5146 in /home/dave/.stack/snapshots/x86_64-linux/69ff23927eb14af3b41d175c1e46b702c42b35820aa90235d4330ac823dfc620/9.6.7/lib/x86_64-linux-ghc-9.6.7/libHSlibtorch-ffi-2.0.1.9-34byZ6iVy6G8ACHBuP71tW-ghc9.6.7.so)\\nframe #14: <unknown function> + 0x108992a (0x76d4e948992a in /home/dave/.stack/snapshots/x86_64-linux/69ff23927eb14af3b41d175c1e46b702c42b35820aa90235d4330ac823dfc620/9.6.7/lib/x86_64-linux-ghc-9.6.7/libHSlibtorch-ffi-2.0.1.9-34byZ6iVy6G8ACHBuP71tW-ghc9.6.7.so)\\n\"(Just \"c10::Error\")"
     ]
    }
   ],
   "source": [
    "let firstBlock = head (layers trainedModel)\n",
    "let mhaLayer = attention firstBlock\n",
    "\n",
    "-- We need to tokenize a prompt manually here since we aren't using the full 'generate' loop\n",
    "let prompt = \"The wizard cast a spell\"\n",
    "let tokens = T.words (T.pack prompt)\n",
    "let indices = map (\\w -> fromMaybe 0 (OSet.findIndex w vocab)) tokens\n",
    "\n",
    "-- Add batch dimension: [1, SeqLen]\n",
    "let inputTensorLong = asTensor [indices]\n",
    "\n",
    "-- Embed it: [1, SeqLen, 64]\n",
    "let dummyInput = forwardEmbedding trainedModel inputTensorLong\n",
    "\n",
    "-- Run the Debug Forward Pass\n",
    "let (output, weights) = forwardMHADebug mhaLayer dummyInput\n",
    "\n",
    "-- Check the shape of weights: Should be [2, 4, 10, 10] \n",
    "-- (Batch=2, Heads=4, Seq=10, Seq=10)\n",
    "print (Th.shape weights)\n",
    "\n",
    "-- 2. Slice out ONE attention map\n",
    "-- Select Batch 0\n",
    "let batch0 = Th.select 0 0 weights \n",
    "-- Select Head 0\n",
    "let head0 = Th.select 0 0 batch0 \n",
    "\n",
    "-- Shape should now be [10, 10]\n",
    "print (Th.shape head0)\n",
    "\n",
    "-- 3. Visualize!\n",
    "visualizeAttention head0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0712db7d-cb23-4ca0-b3ae-4c7d4f5966ac",
   "metadata": {},
   "source": [
    "### 4. Step-by-Step Gradient Watch\n",
    "Your training loop prints loss every 50 iterations.\n",
    "In Jupyter, you can run one single training step and inspect the gradients manually to check for \"Exploding Gradients\" or \"Dead Neurons\" (gradients of 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dded3d-2d25-49c9-a661-211db85afe1f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Haskell",
   "language": "haskell",
   "name": "haskell"
  },
  "language_info": {
   "codemirror_mode": "ihaskell",
   "file_extension": ".hs",
   "mimetype": "text/x-haskell",
   "name": "haskell",
   "pygments_lexer": "Haskell",
   "version": "9.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
